{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcFON4fV7+Aj4HxSGhdgjL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShareAndShine/Lemalabs/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5V1EwtLuNIo"
      },
      "source": [
        "s1 = 'My name is ------------------ Pawan & *#Raj'\n",
        "s2 = 'Today is very ****** good day'\n",
        "s3 = 'How can I _ help you ?'"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u846b-d1vbhs"
      },
      "source": [
        "# Step 1 - convert text into lowercase\n",
        "s1 = s1.lower() # In NLP step 1 is always convert into lower case then tokenize and clean\n",
        "s2 = s2.lower()\n",
        "s3 = s3.lower()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skwBN2P1wOjc",
        "outputId": "babed8ac-404f-481e-f32a-e95953fff8b4"
      },
      "source": [
        "# Step 2 - Tokenize \n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-AuRlecwbLI"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer('\\w+')\n",
        "\n",
        "def tok(x): # helper method to split words in the sentence and holds in an array\n",
        "  return (tokenizer.tokenize(x))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZjTup1RwyX0",
        "outputId": "87fe19bb-c412-4d73-8235-20e7f34bfb67"
      },
      "source": [
        "print(tok(s1))\n",
        "print(tok(s2))\n",
        "print(tok(s3))\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['my', 'name', 'is', 'pawan', 'raj']\n",
            "['today', 'is', 'very', 'good', 'day']\n",
            "['how', 'can', 'i', '_', 'help', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWh1ZJZKx6M0",
        "outputId": "c5263b6b-d0d9-4bac-e43d-885c1432921a"
      },
      "source": [
        "s1 = tok(s1)\n",
        "s2 = tok(s2)\n",
        "s3 = tok(s3)\n",
        "s1"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my', 'name', 'is', 'pawan', 'raj']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIIC65auzIYq",
        "outputId": "148eb955-2555-4c46-ce76-402eaced296a"
      },
      "source": [
        "# Step 3 - Cleaning\n",
        "\n",
        "# Remove any spill over special characters from the above step which do not add any value\n",
        "\n",
        "import re # import regular expression python lib \n",
        "\n",
        "pattern =\"_\"\n",
        "\n",
        "s3 = [re.sub(pattern,'',i) for i in s3]\n",
        "s3"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['how', 'can', 'i', '', 'help', 'you']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMdWhMY902ST"
      },
      "source": [
        "**Sample **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lqE4DppzwdL",
        "outputId": "aa7cd15f-a606-4b60-8b3a-7d2c83a7d38a"
      },
      "source": [
        "ex = 'I am _ Rajesh 15'\n",
        "\n",
        "ex = tok(ex.lower())\n",
        "ex\n",
        "\n",
        "# Remove special characters\n",
        "pattern = \"_\"\n",
        "ex = [re.sub(pattern,'',i) for i in ex]\n",
        "ex\n",
        "\n",
        "# Remove numbers\n",
        "pattern = \"[0-9]\"\n",
        "ex = [re.sub(pattern,'',i) for i in ex]\n",
        "ex\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', '', 'rajesh', '']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQB5zxZu1Whk",
        "outputId": "9c332e18-6420-483f-d5f1-c7b862cf1907"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNW1Ojut1mRK",
        "outputId": "32c944ed-8dd2-4366-a0cd-5f1c2642db23"
      },
      "source": [
        "sw = stopwords.words('english')\n",
        "sw # lists all stops words"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLbyK_Mn2QXY",
        "outputId": "8c637bca-a8a4-485b-d2d4-87a39cc060fb"
      },
      "source": [
        "s1 = [ word for word in s1 if word not in sw]\n",
        "s2 = [ word for word in s2 if word not in sw]\n",
        "s3 = [ word for word in s3 if word not in sw]\n",
        "s3"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', 'help']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzcSAh-L6aQw"
      },
      "source": [
        "**# LETS DO ALL THE ABOVE STEPS IN ONE GO WITH SKLEARN ALGORITHUM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrCgxkNA55_h",
        "outputId": "ebed6a43-71c5-481b-c92d-27a5e4f0f808"
      },
      "source": [
        "\n",
        "\n",
        "s1 = 'Send us your password'\n",
        "s2 = 'Send us your review'\n",
        "s3 = 'Review your password'\n",
        "s4 = 'Review us'\n",
        "s5 = 'Send your password'\n",
        "s6 = 'Send us your account'\n",
        "\n",
        "# create an array to hold all sentences\n",
        "x = [s1, s2, s3, s4, s5, s6]\n",
        "x"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Send us your password',\n",
              " 'Send us your review',\n",
              " 'Review your password',\n",
              " 'Review us',\n",
              " 'Send your password',\n",
              " 'Send us your account']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KiXKX2W7Xhe"
      },
      "source": [
        "def process_text(text): # helper function to remove special chars and numbers\n",
        "   #Remove special characters\n",
        "  pattern = \"_\"\n",
        "  text = [re.sub(pattern,'',i) for i in text]\n",
        "  \n",
        "  # Remove numbers\n",
        "  pattern1 = \"[0-9]\"\n",
        "  text = [re.sub(pattern,'',i) for i in text]\n",
        "  return text\n",
        "  "
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spZfa92V7xzg",
        "outputId": "04a45d79-add9-4b82-bb42-aba75d4cd562"
      },
      "source": [
        "x = process_text(x)\n",
        "x"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Send us your password',\n",
              " 'Send us your review',\n",
              " 'Review your password',\n",
              " 'Review us',\n",
              " 'Send your password',\n",
              " 'Send us your account']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42ES69gO7xqd",
        "outputId": "ef7983d8-191a-42b5-8749-3e680b27c7fb"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "Vectorizer = CountVectorizer(tokenizer=tok, stop_words=sw, ngram_range=(1,1)) # send all functions defined above as an argument to vectorizer function\n",
        "\n",
        "Vector_x = Vectorizer.fit_transform(x) # Forms a bag of words matrix \n",
        "print(Vector_x.toarray())"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0 1 1]\n",
            " [0 0 1 1 1]\n",
            " [0 1 1 0 0]\n",
            " [0 0 1 0 1]\n",
            " [0 1 0 1 0]\n",
            " [1 0 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyAds7AW9N4Y",
        "outputId": "0e103299-13ab-48bd-9b31-2abd33b473cb"
      },
      "source": [
        "Vectorizer.get_feature_names() # if you wonder what unique words or features on which bag of words matrix was built..use this"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['account', 'password', 'review', 'send', 'us']"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPQoqlfDAgOI",
        "outputId": "c60958a9-bc88-4739-bd23-fad35cf2f94f"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "Tfid = TfidfTransformer()\n",
        "\n",
        "Vector_x = Tfid.fit_transform(Vector_x) # closest to 1 signifies word is important as repeated in many places \n",
        "print(Vector_x.toarray())"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.63646032 0.         0.54539814 0.54539814]\n",
            " [0.         0.         0.63646032 0.54539814 0.54539814]\n",
            " [0.         0.70710678 0.70710678 0.         0.        ]\n",
            " [0.         0.         0.7593387  0.         0.65069558]\n",
            " [0.         0.7593387  0.         0.65069558 0.        ]\n",
            " [0.76608386 0.         0.         0.45448626 0.45448626]]\n"
          ]
        }
      ]
    }
  ]
}